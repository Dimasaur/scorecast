# -*- coding: utf-8 -*-
"""review_score_model_v0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11ieBgCUyRB5MDPt_Mn_T_6kCruwxuMAM
"""

import pandas as pd

from google.colab import drive
drive.mount('/content/drive')

# Creating a destination path
file_path = "/content/Pre-processing 1_restaurants_limited_features.csv"
df_restaurants = pd.read_csv(file_path)

"""# 1 Cleaning dataset

## 1.1 Getting rid of old index columns
"""

df_restaurants.head()
df_restaurants = df_restaurants.drop(columns=['Unnamed: 0', 'index'])

"""## 1.2 Ensuring we are using consistent datatypes (boolean)

### 1.2.1 First we check the datatypes, unique values, and value counts
"""

df_restaurants.dtypes

for column in df_restaurants.columns:
    unique_values = df_restaurants[column].unique()
    print(f"Unique values in '{column}':\n{unique_values}\n")

# Display value counts for each column in the DataFrame
for column in df_restaurants.columns:
    print(f"Value counts for '{column}':")
    print(df_restaurants[column].value_counts(dropna=False))
    print("\n")

"""### 1.2.2 We will change string True/False to booleans, change is_open to boolean, and assume that 'none' = 'false'"""

# Step 1: Convert 'is_open' to a boolean
df_restaurants['is_open'] = df_restaurants['is_open'].astype(bool)

# Step 2: Define the columns that should have boolean values
bool_columns = [
    'delivery', 'alcohol', 'bike_parking', 'credit_card', 'appointment_only',
    'caters', 'coat_check', 'dogs', 'drive_thru', 'good_for_kids',
    'good_for_groups', 'happy_hour', 'tv', 'outdoor_seating',
    'reservations', 'table_service', 'take_out', 'wheelchair'
]

# Replace 'True'/'False' strings with actual boolean values and handle 'None'
for column in bool_columns:
    df_restaurants[column] = df_restaurants[column].replace({'True': True, 'False': False, 'None': None})

# Step 3: Clean the 'price_range' column
df_restaurants['price_range'] = pd.to_numeric(df_restaurants['price_range'], errors='coerce')

# Step 4: Replace any remaining None (NaN) values in boolean columns with False
df_restaurants[bool_columns] = df_restaurants[bool_columns].fillna(False)

"""Now we re-verify the value counts"""

df_restaurants.dtypes

# Display value counts for each cleaned column
for column in df_restaurants.columns:
    print(f"Value counts for '{column}' after cleaning:")
    print(df_restaurants[column].value_counts(dropna=False))
    print("\n")

# Drop rows where 'price_range' is NaN
df_restaurants = df_restaurants.dropna(subset=['price_range'])

# Verify that the rows have been dropped
print(df_restaurants['price_range'].value_counts(dropna=False))

"""# 2 Checking for correlation between features and review score

## 2.1 Iniital correlation tests
"""

df_restaurants_model = df_restaurants.drop(columns=['latitude', 'longitude', 'review_count', 'is_open', 'business_id', 'postal_code', 'food_type'])

"""We will exclude a few of the columns that we don't want to test (because they are not really features)

Open question: should we filter out restaurants that have less than some threshold of review count?

Other open question: What should we do about is_open (if anything)?
"""

# Calculate the correlation of all features with the 'stars' column
correlation_table = df_restaurants_model.corr()['stars'].round(2)

# Display the correlation table
print(correlation_table)

# Calculate the correlation matrix
correlation_matrix = df_restaurants_model.corr().round(2)

# Display the correlation matrix
print(correlation_matrix)

import matplotlib.pyplot as plt
import seaborn as sns

# Calculate the correlation matrix
correlation_matrix = df_restaurants_model.corr().round(2)

# Create a heatmap with adjusted size and smaller annotation text
fig = plt.figure(figsize=(12, 10))  # Increase the figure size
sns.heatmap(correlation_matrix, cmap="coolwarm", annot=True, annot_kws={"size":8}, fmt='.2f')

# Display the plot
# plt.show()

"""# 3 Initial regression (without any modifications)"""

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import pandas as pd
from sklearn.preprocessing import PolynomialFeatures
import statsmodels.api as sm


# Step 1: Prepare the data
# Ensure all boolean columns are numeric (True/False -> 1/0)
df_restaurants_model = df_restaurants_model.astype({col: 'int' for col in df_restaurants_model.select_dtypes(include=['bool']).columns})

# Handle categorical variables (if any) using one-hot encoding
df_restaurants_model = pd.get_dummies(df_restaurants_model, drop_first=True)

# Drop any rows with missing values (if not done already)
df_restaurants_model = df_restaurants_model.dropna()

# Step 2: Split the data into features (X) and target (y)
X = df_restaurants_model.drop(columns=['stars'], errors='ignore')
y = df_restaurants_model['stars']  # Target

# Add a constant to the model (intercept)
X = sm.add_constant(X)

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 3: Perform the regression using statsmodels
# Fit the model to the training data
model = sm.OLS(y_train, X_train).fit()

# Step 4: Get the summary of the model
print(model.summary())

# Step 5: Make predictions on the test set (optional)
y_pred = model.predict(X_test)

# Evaluate the model on the test set (optional)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error on Test Set: {mse}")
print(f"R-squared on Test Set: {r2}")

"""# 4 Improving regression

## 4.1 Checking for multicollinearity
"""

from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant
import numpy as np
import pandas as pd

X_for_vif = X.drop(columns=['const'], errors='ignore')

# Ensure that the target variable 'stars' is not in the feature set
assert 'stars' not in X_for_vif.columns, "'stars' should not be in the feature set for VIF calculation."

# Initialize an empty DataFrame to store VIF results
vif_data = pd.DataFrame()
vif_data["Feature"] = X_for_vif.columns
vif_data["VIF"] = [variance_inflation_factor(X_for_vif.values, i) for i in range(X_for_vif.shape[1])]

# Display the VIF data, filtering out features with low multicollinearity (VIF < 5)
vif_filtered = vif_data[vif_data["VIF"] > 5].sort_values(by="VIF", ascending=False)
print(vif_filtered)

"""Let's check what these are collinear with again:

*   take_out 0.21 with delivery and 0.28 with good_for_kids
*   credit_card doesn't have a particularly high correlation with any other variables
*   price_range 0.46 with alcohol, 0.46 reservations, 0.30 with table_service, 0.19 with happy_hour, -0.24 with drive_thru
*   good_for_groups 0.31 with alcohol, 0.31 with good_for_groups, 0.26 with tv, 0.25 with reservations
*   good_for_kids 0.31 with good_for_groups, 0.28 with take_out, 0.19 with price_range

Since we may want to keep some of these we might want to apply regularization to reduce multicollinearity issues
"""

# Create a heatmap with adjusted size and smaller annotation text
fig = plt.figure(figsize=(12, 10))  # Increase the figure size
sns.heatmap(correlation_matrix, cmap="coolwarm", annot=True, annot_kws={"size":8}, fmt='.2f')

# Display the plot
# plt.show()

"""## 4.2 Training random forest model to check feature importance"""

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# Initialize the Random Forest Regressor
rf = RandomForestRegressor(n_estimators=100, random_state=42)

# Train the model
rf.fit(X_train, y_train)

# Make predictions on the test set
y_pred = rf.predict(X_test)

# Calculate the mean squared error
mse = mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error: {mse}")

# Calculate feature importances
importances = rf.feature_importances_
feature_importance = pd.DataFrame({'Feature': X.columns, 'Importance': importances})

# Sort by importance
feature_importance = feature_importance.sort_values(by='Importance', ascending=False)

# Display the feature importance
print(feature_importance)

"""Suggested next steps:

*  Drop lowest importance features: appointment_only, coat_check, dogs, happy_hour. dogs
*   Think about interaction terms for moderate_important features (0.01 to 0.1)
*   Fix scaling and encoding
*   Retrain model and compare performance (using cross-validation)
*   Apply regularization

## 4.3 Testing interaction terms
"""

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import pandas as pd
from sklearn.preprocessing import PolynomialFeatures

# Step 1: Remove any constant (intercept) before creating polynomial features
X_no_const = X.drop(columns=['const'], errors='ignore')

# Step 2: Create interaction terms using PolynomialFeatures without the constant
poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)
X_poly = poly.fit_transform(X_no_const)

# Get the names of the features including interaction terms
feature_names = poly.get_feature_names_out(input_features=X_no_const.columns)

# Convert the array back to a DataFrame
X_poly_df = pd.DataFrame(X_poly, columns=feature_names)

# Step 3: Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_poly_df, y, test_size=0.2, random_state=42)

# Step 4: Fit a model (e.g., RandomForest) to evaluate feature importance
model = RandomForestRegressor(random_state=42)
model.fit(X_train, y_train)

# Step 5: Get the feature importance scores
importance_scores = model.feature_importances_
feature_importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': importance_scores
}).sort_values(by='Importance', ascending=False)

print(feature_importance_df)

# Step 6: Evaluate the model with interaction terms
y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")

# Convert the Importance values to a readable format with six decimal places
feature_importance_df['Importance'] = feature_importance_df['Importance'].apply(lambda x: f'{x:.6f}')

# Replacing spaces between feature names with ' * ' to indicate interaction
feature_importance_df['Feature'] = feature_importance_df['Feature'].apply(lambda x: ' * '.join(x.split()))

# Display the formatted DataFrame
print(feature_importance_df.to_string(index=False))

"""## 4.4 Re-running regression after dropping some features"""

# Dropping: appointment_only, coat_check, dogs, good_for_groups
df_restaurants_model['delivery_drive_thru'] = df_restaurants_model['delivery'] * df_restaurants_model['drive_thru']
X_reduced = df_restaurants_model.drop(columns=['stars', 'coat_check', 'good_for_groups', 'credit_card', 'reservations', 'coat_check', 'appointment_only', 'drive_thru'], errors='ignore')

# Add a constant to the model (intercept)
X_reduced = sm.add_constant(X_reduced)

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.2, random_state=42)

# Step 3: Perform the regression using statsmodels
# Fit the model to the training data
model = sm.OLS(y_train, X_train).fit()

# Step 4: Get the summary of the model
print(model.summary())

# Step 5: Make predictions on the test set (optional)
y_pred = model.predict(X_test)

# Evaluate the model on the test set (optional)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error on Test Set: {mse}")
print(f"R-squared on Test Set: {r2}")

X_for_vif = X_reduced.drop(columns=['const'], errors='ignore')

# Ensure that the target variable 'stars' is not in the feature set
assert 'stars' not in X_for_vif.columns, "'stars' should not be in the feature set for VIF calculation."

# Initialize an empty DataFrame to store VIF results
vif_data = pd.DataFrame()
vif_data["Feature"] = X_for_vif.columns
vif_data["VIF"] = [variance_inflation_factor(X_for_vif.values, i) for i in range(X_for_vif.shape[1])]

# Display the VIF data, filtering out features with low multicollinearity (VIF < 5)
vif_filtered = vif_data[vif_data["VIF"] > 5].sort_values(by="VIF", ascending=False)
print(vif_filtered)

correlation_matrix = df_restaurants_model.corr().round(2)

# Create a heatmap with adjusted size and smaller annotation text
fig = plt.figure(figsize=(12, 10))  # Increase the figure size
sns.heatmap(correlation_matrix, cmap="coolwarm", annot=True, annot_kws={"size":8}, fmt='.2f')

# Display the plot
# plt.show()

# Initialize the Random Forest Regressor
rf = RandomForestRegressor(n_estimators=100, random_state=42)

# Train the model
rf.fit(X_train, y_train)

# Make predictions on the test set
y_pred = rf.predict(X_test)

# Calculate the mean squared error
mse = mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error: {mse}")

# Calculate feature importances
importances = rf.feature_importances_

# Ensure to use the feature names from the transformed dataset
feature_importance = pd.DataFrame({'Feature': X_train.columns, 'Importance': importances})

# Sort by importance
feature_importance = feature_importance.sort_values(by='Importance', ascending=False)

# Display the feature importance
print(feature_importance)

"""## 4.5 Using recursive feature elimination"""

from sklearn.feature_selection import RFE
from sklearn.linear_model import LinearRegression

model = LinearRegression()
rfe = RFE(model, n_features_to_select=10)
rfe = rfe.fit(X_train, y_train)

print(f"Selected Features: {X_train.columns[rfe.support_]}")

selected_features = X_train.columns[rfe.support_]
X_train_selected = X_train[selected_features]
X_test_selected = X_test[selected_features]
X_selected = X_train[selected_features]

# Retrain the model with selected features
model.fit(X_train_selected, y_train)
y_pred_selected = model.predict(X_test_selected)

# Evaluate the model
mse_selected = mean_squared_error(y_test, y_pred_selected)
r2_selected = r2_score(y_test, y_pred_selected)

print(f"Mean Squared Error with Selected Features: {mse_selected}")
print(f"R-squared with Selected Features: {r2_selected}")

from sklearn.model_selection import cross_val_score

cv_scores = cross_val_score(model, X_train_selected, y_train, cv=5)
print(f"Cross-Validation Scores with Selected Features: {cv_scores}")

"""# 5 Testing different **models**

1. Use SGDRegressor and perform GridSearchCV to fine-tune 'penalty' and 'alpha'
2. Implement KNeighborsRegressor with GridSearchCV to optimize hyperparameters
3. Evaluate the model using RandomForestRegressor
4. Test GradientBoostingRegressor for potential enhancements
5. Explore XGBoostRegressor as a more advanced boosting method
6. Experiment with SVR and try different kernel options

## 5.1 Setup: Model Parameters and GCS Storage
"""

import numpy as np
import os
import joblib
import pandas as pd
from datetime import datetime
from google.cloud import storage
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import GridSearchCV

# Set GCP bucket name
BUCKET_NAME = 'flavor-forecast'
MODEL_PATH = 'models/flavor-forecast-models'

# Initialize Google Cloud Storage client
client = storage.Client()

# Function to save model to GCS
def save_model_to_gcs(model, model_name):
    model_filename = f"{model_name}.joblib"
    joblib.dump(model, model_filename)

    gcs_model_path = f"{MODEL_PATH}{model_filename}"

    bucket = client.bucket(BUCKET_NAME)
    blob = bucket.blob(gcs_model_path)
    blob.upload_from_filename(model_filename)

    os.remove(model_filename)
    print(f"Model saved to GCS bucket at {gcs_model_path}")

# Function to append results to CSV
def append_results_to_csv(model_name, r_squared, results_csv='model_results.csv'):
    date_trained = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    csv_filename = f"{model_name}.csv"

    if os.path.exists(results_csv):
        df_results = pd.read_csv(results_csv)
    else:
        df_results = pd.DataFrame(columns=['date_trained', 'csv_filename', 'r_squared'])

    new_row = {'date_trained': date_trained, 'csv_filename': csv_filename, 'r_squared': r_squared}
    df_results = df_results.append(new_row, ignore_index=True)

    df_results.to_csv(results_csv, index=False)
    print(f"Results appended to {results_csv}")

X_train_sel, X_test_sel, y_train_sel, y_test_sel = train_test_split(X_selected, y, test_size=0.2, random_state=42)

"""## 5.2 Define Model Parameters for Each Model



"""

# Define parameter grids for each model
model_configs = {
    'SGDRegressor': {
        'model': SGDRegressor(random_state=42),
        'params': {
            'alpha': [0.0001, 0.001, 0.01, 0.1],
            'penalty': ['l2', 'l1', 'elasticnet'],
            'max_iter': [1000, 2000, 5000],
            'learning_rate': ['constant', 'optimal', 'invscaling', 'adaptive'],
            'eta0': [0.01, 0.1, 1.0]
        }
    },
    'KNeighborsRegressor': {
        'model': KNeighborsRegressor(),
        'params': {
            'n_neighbors': [3, 5, 7, 10],
            'weights': ['uniform', 'distance'],
            'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],
        }
    },
    'RandomForestRegressor': {
        'model': RandomForestRegressor(random_state=42),
        'params': {
            'n_estimators': [100, 200, 500],
            'max_features': ['auto', 'sqrt', 'log2'],
            'max_depth': [10, 20, 30, None],
            'min_samples_split': [2, 5, 10],
            'min_samples_leaf': [1, 2, 4],
        }
    },
    'GradientBoostingRegressor': {
        'model': GradientBoostingRegressor(random_state=42),
        'params': {
            'n_estimators': [100, 200, 500],
            'learning_rate': [0.01, 0.1, 0.05],
            'max_depth': [3, 5, 7],
            'subsample': [0.8, 0.9, 1.0],
        }
    },
    'XGBoostRegressor': {
        'model': XGBRegressor(random_state=42),
        'params': {
            'n_estimators': [100, 200, 500],
            'learning_rate': [0.01, 0.1, 0.05],
            'max_depth': [3, 5, 7],
            'subsample': [0.8, 0.9, 1.0],
        }
    },
    'SVMRegressor': {
        'model': SVR(),
        'params': {
            'C': [0.1, 1, 10, 100],
            'gamma': ['scale', 'auto'],
            'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],
        }
    }
}

"""## 5.3 Train and Save Each Model"""

best_overall_model = None
best_r2_score = float('-inf')
best_model_name = ""

for model_name, config in model_configs.items():
    print(f"Training {model_name}...")
    grid_search = GridSearchCV(config['model'], config['params'], cv=5, scoring='r2', n_jobs=-1)
    grid_search.fit(X_train_sel, y_train_sel)

    best_model = grid_search.best_estimator_
    y_pred = best_model.predict(X_test_sel)
    r2 = r2_score(y_test_sel, y_pred)

    # Save the model to GCS
    save_model_to_gcs(best_model, model_name)

    # Append results to the CSV
    append_results_to_csv(model_name, r2)

    # Track the best model overall
    if r2 > best_r2_score:
        best_r2_score = r2
        best_overall_model = best_model
        best_model_name = model_name

print(f"Best overall model: {best_model_name} with R-squared: {best_r2_score}")

"""## Archive

First we run a grid search to optimize parameters
"""

#param_grid = {
#    'alpha': [0.0001, 0.001, 0.01, 0.1],
#    'penalty': ['l2', 'l1', 'elasticnet'],
#    'max_iter': [1000, 2000, 5000],
#    'learning_rate': ['constant', 'optimal', 'invscaling', 'adaptive'],
#    'eta0': [0.01, 0.1, 1.0]
#}

"""We want to do this for X_reduced first"""

# Split your data into training and testing sets
#X_train_red, X_test_red, y_train_red, y_test_red = train_test_split(X_reduced, y, test_size=0.2, random_state=42)

# Initialize the SGDRegressor
#sgd = SGDRegressor(random_state=42)

# Set up the grid search
#grid_search_red = GridSearchCV(sgd, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)

# Fit the model on the reduced dataset
#grid_search_red.fit(X_train_red, y_train_red)

# Get the best model and parameters
#best_model_red = grid_search_red.best_estimator_
#print("Best parameters for X_reduced:", grid_search_red.best_params_)

# Evaluate the model
#y_pred_red = best_model_red.predict(X_test_red)
#mse_red = mean_squared_error(y_test_red, y_pred_red)
#r2_red = r2_score(y_test_red, y_pred_red)

#print(f"X_reduced - Mean Squared Error: {mse_red}")
#print(f"X_reduced - R-squared: {r2_red}")

"""And then X_selected (from recursive feature selection)"""

# Split your data into training and testing sets
#X_train_sel, X_test_sel, y_train_sel, y_test_sel = train_test_split(X_selected, y, test_size=0.2, random_state=42)

# Initialize the SGDRegressor
#sgd = SGDRegressor(random_state=42)

# Set up the grid search
#grid_search_sel = GridSearchCV(sgd, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)

# Fit the model on the selected dataset
#grid_search_sel.fit(X_train_sel, y_train_sel)

# Get the best model and parameters
#best_model_sel = grid_search_sel.best_estimator_
#print("Best parameters for X_selected:", grid_search_sel.best_params_)

# Evaluate the model
#y_pred_sel = best_model_sel.predict(X_test_sel)
#mse_sel = mean_squared_error(y_test_sel, y_pred_sel)
#r2_sel = r2_score(y_test_sel, y_pred_sel)

#print(f"X_selected - Mean Squared Error: {mse_sel}")
#print(f"X_selected - R-squared: {r2_sel}")

# Save the model to GCS
#save_model_to_gcs(sgd, 'SGDRegressor')

"""5.2 K-neighbors"""

#from sklearn.neighbors import KNeighborsRegressor

# Initialize the KNeighborsRegressor
#knn = KNeighborsRegressor()

# Define the parameter grid
#param_grid_knn = {
#    'n_neighbors': [3, 5, 7, 10],
#    'weights': ['uniform', 'distance'],
#    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],
#}

# Set up the grid search
#grid_search_knn = GridSearchCV(knn, param_grid_knn, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)

# Fit the model on the selected dataset
#grid_search_knn.fit(X_train_sel, y_train_sel)

# Get the best model and parameters
#best_model_knn = grid_search_knn.best_estimator_
#print("Best parameters for K-Neighbors:", grid_search_knn.best_params_)

# Evaluate the model
#y_pred_knn = best_model_knn.predict(X_test_sel)
#mse_knn = mean_squared_error(y_test_sel, y_pred_knn)
#r2_knn = r2_score(y_test_sel, y_pred_knn)

#print(f"K-Neighbors - Mean Squared Error: {mse_knn}")
#print(f"K-Neighbors - R-squared: {r2_knn}")

# Save the best KNeighbors model to GCS
#save_model_to_gcs(best_knn_model, 'KNeighborsRegressor')

"""5.3 Random forest"""

#from sklearn.ensemble import RandomForestRegressor

# Initialize the RandomForestRegressor
#rf = RandomForestRegressor(random_state=42)

# Define the parameter grid
#param_grid_rf = {
#    'n_estimators': [100, 200, 500],
#    'max_features': ['auto', 'sqrt', 'log2'],
#    'max_depth': [10, 20, 30, None],
#    'min_samples_split': [2, 5, 10],
#    'min_samples_leaf': [1, 2, 4],
#}

# Set up the grid search
#grid_search_rf = GridSearchCV(rf, param_grid_rf, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)

# Fit the model on the selected dataset
#grid_search_rf.fit(X_train_sel, y_train_sel)

# Get the best model and parameters
#best_model_rf = grid_search_rf.best_estimator_
#print("Best parameters for Random Forest:", grid_search_rf.best_params_)

# Evaluate the model
#y_pred_rf = best_model_rf.predict(X_test_sel)
#mse_rf = mean_squared_error(y_test_sel, y_pred_rf)
#r2_rf = r2_score(y_test_sel, y_pred_rf)

#print(f"Random Forest - Mean Squared Error: {mse_rf}")
#print(f"Random Forest - R-squared: {r2_rf}")

# Save the best RandomForestRegressor model to GCS
#save_model_to_gcs(grid_search_rf.best_estimator_, 'RandomForestRegressor')

"""5.4 Gradient-boosting regressor"""

#from sklearn.ensemble import GradientBoostingRegressor

# Initialize the GradientBoostingRegressor
#gbr = GradientBoostingRegressor(random_state=42)

# Define the parameter grid
#param_grid_gbr = {
#    'n_estimators': [100, 200, 500],
#    'learning_rate': [0.01, 0.1, 0.05],
#    'max_depth': [3, 5, 7],
#    'subsample': [0.8, 0.9, 1.0],
#}

# Set up the grid search
#grid_search_gbr = GridSearchCV(gbr, param_grid_gbr, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)

# Fit the model on the selected dataset
#grid_search_gbr.fit(X_train_sel, y_train_sel)

# Get the best model and parameters
#best_model_gbr = grid_search_gbr.best_estimator_
#print("Best parameters for Gradient Boosting:", grid_search_gbr.best_params_)

# Evaluate the model
#y_pred_gbr = best_model_gbr.predict(X_test_sel)
#mse_gbr = mean_squared_error(y_test_sel, y_pred_gbr)
#r2_gbr = r2_score(y_test_sel, y_pred_gbr)

#print(f"Gradient Boosting - Mean Squared Error: {mse_gbr}")
#print(f"Gradient Boosting - R-squared: {r2_gbr}")

# Save the best GradientBoostingRegressor model to GCS
#save_model_to_gcs(grid_search_gbr.best_estimator_, 'GradientBoostingRegressor')

"""5.5 Xgboost"""

#from xgboost import XGBRegressor

# Initialize the XGBRegressor
#xgb = XGBRegressor(random_state=42)

# Define the parameter grid
#param_grid_xgb = {
#    'n_estimators': [100, 200, 500],
#    'learning_rate': [0.01, 0.1, 0.05],
#    'max_depth': [3, 5, 7],
#    'subsample': [0.8, 0.9, 1.0],
#}

# Set up the grid search
#grid_search_xgb = GridSearchCV(xgb, param_grid_xgb, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)

# Fit the model on the selected dataset
#grid_search_xgb.fit(X_train_sel, y_train_sel)

# Get the best model and parameters
#best_model_xgb = grid_search_xgb.best_estimator_
#print("Best parameters for XGBoost:", grid_search_xgb.best_params_)

# Evaluate the model
#y_pred_xgb = best_model_xgb.predict(X_test_sel)
#mse_xgb = mean_squared_error(y_test_sel, y_pred_xgb)
#r2_xgb = r2_score(y_test_sel, y_pred_xgb)

#print(f"XGBoost - Mean Squared Error: {mse_xgb}")
#print(f"XGBoost - R-squared: {r2_xgb}")

# Save the best XGBRegressor model to GCS
#save_model_to_gcs(grid_search_xgb.best_estimator_, 'XGBoostRegressor')

"""5.6 SVM"""

#from sklearn.svm import SVR

# Initialize the SVR
#svr = SVR()

# Define the parameter grid
#param_grid_svr = {
#    'C': [0.1, 1, 10, 100],
#    'gamma': ['scale', 'auto'],
#    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],
#}

# Set up the grid search
#grid_search_svr = GridSearchCV(svr, param_grid_svr, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)

# Fit the model on the selected dataset
#grid_search_svr.fit(X_train_sel, y_train_sel)

# Get the best model and parameters
#best_model_svr = grid_search_svr.best_estimator_
#print("Best parameters for SVM:", grid_search_svr.best_params_)

# Evaluate the model
#y_pred_svr = best_model_svr.predict(X_test_sel)
#mse_svr = mean_squared_error(y_test_sel, y_pred_svr)
#r2_svr = r2_score(y_test_sel, y_pred_svr)

#print(f"SVM - Mean Squared Error: {mse_svr}")
#print(f"SVM - R-squared: {r2_svr}")

# Save the best SVR model to GCS
#save_model_to_gcs(grid_search_svr.best_estimator_, 'SVMRegressor')

"""5.7 Optimize across models"""

#models = {
#    'SGD': grid_search_sel,
#    'KNN': grid_search_knn,
#    'Random Forest': grid_search_rf,
#    'Gradient Boosting': grid_search_gbr,
#    'XGBoost': grid_search_xgb,
#    'SVM': grid_search_svr
#}

#best_model = None
#best_score = float('inf')  # For MSE, lower is better, so start with infinity
#best_model_name = ""

#for name, model in models.items():
#    model.fit(X_train_sel, y_train_sel)
#    y_pred = model.best_estimator_.predict(X_test_sel)
#    mse = mean_squared_error(y_test_sel, y_pred)

#    print(f"{name} - Mean Squared Error: {mse}")

#    if mse < best_score:
#        best_score = mse
#        best_model = model.best_estimator_
#        best_model_name = name

#print(f"Best Model: {best_model_name} with MSE: {best_score}")
