{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmaS5cs3-Clp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hL58RmBx-Clq"
      },
      "outputs": [],
      "source": [
        "# Creating a destination path\n",
        "file_path = \"/content/restaurants_preprocessed.csv\"\n",
        "\n",
        "df_restaurants = pd.read_csv(file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiIPSuS6-Clr"
      },
      "source": [
        "# 1 Cleaning dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eQlFD08-Cls"
      },
      "source": [
        "## 1.1 Getting rid of old index columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "-HA_1xM_-Cls",
        "outputId": "6831afdf-4293-4336-f766-f1fdfdaeb607"
      },
      "outputs": [],
      "source": [
        "df_restaurants.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDnfLmm8-Cls"
      },
      "source": [
        "## 1.2 Ensuring we are using consistent datatypes (boolean)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_Mt4dVI-Cls"
      },
      "source": [
        "### 1.2.1 First we check the datatypes, unique values, and value counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zP2MCEIX-Cls",
        "outputId": "e6f84b06-a019-4778-c199-f664df835d58"
      },
      "outputs": [],
      "source": [
        "df_restaurants.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7-LGa1d-Cls",
        "outputId": "a9a9c4dd-1891-488c-d8e8-0ab2972322c0"
      },
      "outputs": [],
      "source": [
        "for column in df_restaurants.columns:\n",
        "    unique_values = df_restaurants[column].unique()\n",
        "    print(f\"Unique values in '{column}':\\n{unique_values}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBujHbdc-Cls",
        "outputId": "3f629b1d-ce66-4f53-bb20-6721e8983150"
      },
      "outputs": [],
      "source": [
        "# Display value counts for each column in the DataFrame\n",
        "for column in df_restaurants.columns:\n",
        "    print(f\"Value counts for '{column}':\")\n",
        "    print(df_restaurants[column].value_counts(dropna=False))\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRrwBsgy-Clt"
      },
      "source": [
        "### 1.2.2 We will change string True/False to booleans, change is_open to boolean, and assume that 'none' = 'false'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvVBYgDZ-Clt",
        "outputId": "15affb95-3589-4bf7-d710-f7254e0bdca6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Assuming df_restaurants is your DataFrame\n",
        "\n",
        "# Step 1: Convert 'is_open' to boolean\n",
        "df_restaurants['is_open'] = df_restaurants['is_open'].astype(bool)\n",
        "\n",
        "# Step 2: Clean up boolean columns\n",
        "bool_columns = [\n",
        "    'delivery', 'alcohol', 'bike_parking', 'credit_card', 'appointment_only',\n",
        "    'caters', 'coat_check', 'dogs', 'drive_thru', 'good_for_kids',\n",
        "    'good_for_groups', 'happy_hour', 'tv', 'outdoor_seating',\n",
        "    'reservations', 'table_service', 'take_out', 'wheelchair'\n",
        "]\n",
        "\n",
        "# Replace 'True'/'False' strings with actual boolean values and None with np.nan\n",
        "for column in bool_columns:\n",
        "    df_restaurants[column] = df_restaurants[column].replace({'True': True, 'False': False, 'None': np.nan})\n",
        "\n",
        "# Step 3: Clean the 'price_range' column and handle NaN\n",
        "# Convert any remaining non-numeric values (e.g., 'False') to NaN in 'price_range'\n",
        "df_restaurants['price_range'] = pd.to_numeric(df_restaurants['price_range'], errors='coerce')\n",
        "\n",
        "# Apply KNN Imputer on 'price_range'\n",
        "knn_imputer = KNNImputer(n_neighbors=5)\n",
        "df_restaurants[['price_range']] = knn_imputer.fit_transform(df_restaurants[['price_range']])\n",
        "\n",
        "# Round the imputed values to the nearest integer and clip to valid price levels (1-4)\n",
        "df_restaurants['price_range'] = df_restaurants['price_range'].round().clip(1, 4)\n",
        "\n",
        "# Step 4: Replace any remaining NaN values in boolean columns with False\n",
        "df_restaurants[bool_columns] = df_restaurants[bool_columns].fillna(False)\n",
        "\n",
        "# Step 5: Impute the most common value for 'open_on_weekend'\n",
        "# Get the most common value (mode)\n",
        "most_common_value = df_restaurants['open_on_weekend'].mode()[0]\n",
        "df_restaurants['open_on_weekend'].fillna(most_common_value, inplace=True)\n",
        "\n",
        "# Step 6: Standardize 'hours_per_week'\n",
        "scaler = StandardScaler()\n",
        "df_restaurants['hours_per_week'] = scaler.fit_transform(df_restaurants[['hours_per_week']])\n",
        "\n",
        "# Final Check: Display the cleaned and transformed data\n",
        "print(df_restaurants[['price_range', 'hours_per_week', 'open_on_weekend']].head())\n",
        "\n",
        "# Optional: Check the distribution of 'open_on_weekend' after imputation\n",
        "print(\"Value counts for 'open_on_weekend':\")\n",
        "print(df_restaurants['open_on_weekend'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKh3xDOI-Clt"
      },
      "source": [
        "Now we re-verify the value counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "K9NYBjLn-Clt",
        "outputId": "8994cc21-803f-410e-abea-b0bf155800af"
      },
      "outputs": [],
      "source": [
        "df_restaurants.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9hqyqBNC89E"
      },
      "source": [
        "We should check distribution of review_score by review_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "id": "AiTl-kK_C8Yb",
        "outputId": "b70fd306-3eed-49ec-f578-4340feaeab58"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming df_restaurants is your DataFrame\n",
        "\n",
        "# Scatter plot to visualize the relationship between stars and review_count\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x='stars', y='review_count', data=df_restaurants)\n",
        "plt.title('Scatter Plot of Stars by Review Count')\n",
        "plt.xlabel('Review Count')\n",
        "plt.ylabel('Stars')\n",
        "plt.show()\n",
        "\n",
        "# Calculate the correlation between stars and review_count\n",
        "correlation = df_restaurants['stars'].corr(df_restaurants['review_count'])\n",
        "print(f'Correlation between stars and review_count: {correlation}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "WwIFap4zDaNt",
        "outputId": "ad95df12-4ebc-4909-9c2e-846e0347ca83"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "\n",
        "# Create the histogram using Plotly\n",
        "fig = px.histogram(df_restaurants, x='review_count', nbins=1000, title='Distribution of Review Count', marginal='rug')\n",
        "\n",
        "# Customize the labels\n",
        "fig.update_layout(\n",
        "    xaxis_title='Review Count',\n",
        "    yaxis_title='Frequency',\n",
        "    title_x=0.5\n",
        ")\n",
        "\n",
        "# Show the plot\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtqPDsffE0PH",
        "outputId": "bed7e644-c344-4593-f3ad-839af4566bd5"
      },
      "outputs": [],
      "source": [
        "# Save the DataFrame as a CSV file\n",
        "df_restaurants.to_csv('df_restaurants_preprocessed_final.csv', index=False)\n",
        "print('DataFrame saved as df_restaurants_preprocessed_final.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "11iyR7EXGaOD",
        "outputId": "81cab45a-a837-41c5-fa68-ec2ebd168ad1"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Download the file\n",
        "files.download('df_restaurants_preprocessed_final.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibnSrlOf-Clt"
      },
      "source": [
        "# 2 Checking for correlation between features and review score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mb549cue-Clt"
      },
      "source": [
        "## 2.1 Iniital correlation tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JimbZSTlEb0_"
      },
      "outputs": [],
      "source": [
        "df_restaurants_model = df_restaurants.drop(columns=['latitude', 'longitude', 'review_count', 'is_open', 'business_id', 'postal_code', 'food_type', 'attire'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Q2_L5F8FJdR"
      },
      "source": [
        "We will exclude a few of the columns that we don't want to test (because they are not really features)\n",
        "\n",
        "Open question: should we filter out restaurants that have less than some threshold of review count?\n",
        "\n",
        "Other open question: What should we do about is_open (if anything)?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5VKUYa9-Clt",
        "outputId": "11edf9d9-adf5-4837-a385-0376d13c41dd"
      },
      "outputs": [],
      "source": [
        "# Calculate the correlation of all features with the 'stars' column\n",
        "correlation_table = df_restaurants_model.corr()['stars'].round(2)\n",
        "\n",
        "# Display the correlation table\n",
        "print(correlation_table)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 967
        },
        "id": "JkGV1Qla-Clu",
        "outputId": "7279bdca-450d-4bbd-807a-77f8ddf80dd9"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "correlation_matrix = df_restaurants_model.corr().round(2)\n",
        "\n",
        "# Create a heatmap with adjusted size and smaller annotation text\n",
        "fig = plt.figure(figsize=(12, 10))  # Increase the figure size\n",
        "sns.heatmap(correlation_matrix, cmap=\"coolwarm\", annot=True, annot_kws={\"size\":8}, fmt='.2f')\n",
        "\n",
        "# Display the plot\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSqb6Ivi-Clu"
      },
      "source": [
        "# 3 Initial regression (without any modifications)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xULayWn7F2hl",
        "outputId": "f24795e5-babe-4a23-8190-adc29621d6e5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import statsmodels.api as sm\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# Step 1: Prepare the data\n",
        "# Ensure all boolean columns are numeric (True/False -> 1/0)\n",
        "df_restaurants_model = df_restaurants_model.astype({col: 'int' for col in df_restaurants_model.select_dtypes(include=['bool']).columns})\n",
        "\n",
        "# Handle categorical variables (if any) using one-hot encoding\n",
        "df_restaurants_model = pd.get_dummies(df_restaurants_model, drop_first=True)\n",
        "\n",
        "# Drop any rows with missing values (if not done already)\n",
        "df_restaurants_model = df_restaurants_model.dropna()\n",
        "\n",
        "# Step 2: Split the data into features (X) and target (y)\n",
        "X = df_restaurants_model.drop(columns=['stars'], errors='ignore')\n",
        "y = df_restaurants_model['stars']  # Target\n",
        "\n",
        "# Add a constant to the model (intercept)\n",
        "X = sm.add_constant(X)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Perform the regression using statsmodels\n",
        "# Fit the model to the training data\n",
        "model = sm.OLS(y_train, X_train).fit()\n",
        "\n",
        "# Step 4: Get the summary of the model\n",
        "print(model.summary())\n",
        "\n",
        "# Step 5: Make predictions on the test set (optional)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model on the test set (optional)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Mean Squared Error on Test Set: {mse}\")\n",
        "print(f\"R-squared on Test Set: {r2}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2ba3YrK-Clu"
      },
      "source": [
        "# 4 Improving regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHVdZayE-Clu"
      },
      "source": [
        "## 4.1 Checking for multicollinearity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9hUoN9A-Clu",
        "outputId": "d3907853-3665-4810-ec2d-8c443f012a06"
      },
      "outputs": [],
      "source": [
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from statsmodels.tools.tools import add_constant\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "X_for_vif = X.drop(columns=['const'], errors='ignore')\n",
        "\n",
        "# Ensure that the target variable 'stars' is not in the feature set\n",
        "assert 'stars' not in X_for_vif.columns, \"'stars' should not be in the feature set for VIF calculation.\"\n",
        "\n",
        "# Initialize an empty DataFrame to store VIF results\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"Feature\"] = X_for_vif.columns\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(X_for_vif.values, i) for i in range(X_for_vif.shape[1])]\n",
        "\n",
        "# Display the VIF data, filtering out features with low multicollinearity (VIF < 5)\n",
        "vif_filtered = vif_data[vif_data[\"VIF\"] > 5].sort_values(by=\"VIF\", ascending=False)\n",
        "print(vif_filtered)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdQyJMn5IC4y"
      },
      "source": [
        "Let's check what these are collinear with again (correlation coefficients below):\n",
        "\n",
        "*   open_on_weekend: 0.40 with hours_per_week\n",
        "*   price_range: 0.39 with reservations, 0.37 with alcohol, -0.27 good_for_kids, 0.25 table_service, 0.25 happy hour\n",
        "*   take_out: 0.33 good_for_kids, 0.29 credit_card, 0.28 delivery\n",
        "*   credit_card: 0.31 good_for_groups, 0.29 take_out, 0.28 good_for_kids,\n",
        "*   good_for_groups: 0.47 good_for_kids, 0.35 alcohol, 0.32 tv, 0.31 credit_card\n",
        "\n",
        "Since we may want to keep some of these we might want to apply regularization to reduce multicollinearity issues"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 967
        },
        "id": "MB3CUTzUIBcy",
        "outputId": "9387d814-0641-4a89-bfd5-72e88cb5fd1d"
      },
      "outputs": [],
      "source": [
        "# Create a heatmap with adjusted size and smaller annotation text\n",
        "fig = plt.figure(figsize=(12, 10))  # Increase the figure size\n",
        "sns.heatmap(correlation_matrix, cmap=\"coolwarm\", annot=True, annot_kws={\"size\":8}, fmt='.2f')\n",
        "\n",
        "# Display the plot\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2p6F8v2JkeI"
      },
      "source": [
        "## 4.2 Training random forest model to check feature importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zh5jLgFIGQ8",
        "outputId": "2180d747-a1fa-4d25-e594-b02c0494c49b"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Initialize the Random Forest Regressor\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# Calculate the mean squared error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "\n",
        "# Calculate feature importances\n",
        "importances = rf.feature_importances_\n",
        "feature_importance = pd.DataFrame({'Feature': X.columns, 'Importance': importances})\n",
        "\n",
        "# Sort by importance\n",
        "feature_importance = feature_importance.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Display the feature importance\n",
        "print(feature_importance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9gFOLtiKIJP"
      },
      "source": [
        "Suggested next steps:\n",
        "\n",
        "*  Drop lowest importance features: appointment_only, coat_check, dogs\n",
        "*   Think about interaction terms for moderate_important features (0.01 to 0.1)\n",
        "*   Retrain model and compare performance (using cross-validation)\n",
        "*   Apply regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_SOrIJoLIrP"
      },
      "source": [
        "## 4.3 Testing interaction terms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yoWBWfZJxU6",
        "outputId": "9701a36e-682c-4c7d-bf79-595cca12733f"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# Step 1: Remove any constant (intercept) before creating polynomial features\n",
        "X_no_const = X.drop(columns=['const'], errors='ignore')\n",
        "\n",
        "# Step 2: Create interaction terms using PolynomialFeatures without the constant\n",
        "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
        "X_poly = poly.fit_transform(X_no_const)\n",
        "\n",
        "# Get the names of the features including interaction terms\n",
        "feature_names = poly.get_feature_names_out(input_features=X_no_const.columns)\n",
        "\n",
        "# Convert the array back to a DataFrame\n",
        "X_poly_df = pd.DataFrame(X_poly, columns=feature_names)\n",
        "\n",
        "# Step 3: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_poly_df, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 4: Fit a model (e.g., RandomForest) to evaluate feature importance\n",
        "model = RandomForestRegressor(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Get the feature importance scores\n",
        "importance_scores = model.feature_importances_\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importance_scores\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(feature_importance_df)\n",
        "\n",
        "# Step 6: Evaluate the model with interaction terms\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "print(f\"R-squared: {r2}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwHNFsFSLXjn",
        "outputId": "45d1102b-1e8d-4e75-a74b-3e27acdcfcb4"
      },
      "outputs": [],
      "source": [
        "# Convert the Importance values to a readable format with six decimal places\n",
        "feature_importance_df['Importance'] = feature_importance_df['Importance'].apply(lambda x: f'{x:.6f}')\n",
        "\n",
        "# Replacing spaces between feature names with ' * ' to indicate interaction\n",
        "feature_importance_df['Feature'] = feature_importance_df['Feature'].apply(lambda x: ' * '.join(x.split()))\n",
        "\n",
        "# Display the formatted DataFrame\n",
        "print(feature_importance_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qd0LsxOX3og"
      },
      "source": [
        "## 4.4 Re-running regression after dropping some features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faVlbQHqLyNh",
        "outputId": "a1b1d438-67ab-4c76-dacf-4018b35adce6"
      },
      "outputs": [],
      "source": [
        "# Dropping\n",
        "X_reduced = df_restaurants_model.drop(columns=['stars', 'appointment_only', 'coat_check', 'dogs'], errors='ignore')\n",
        "\n",
        "# Add a constant to the model (intercept)\n",
        "X_reduced = sm.add_constant(X_reduced)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Perform the regression using statsmodels\n",
        "# Fit the model to the training data\n",
        "model = sm.OLS(y_train, X_train).fit()\n",
        "\n",
        "# Step 4: Get the summary of the model\n",
        "print(model.summary())\n",
        "\n",
        "# Step 5: Make predictions on the test set (optional)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model on the test set (optional)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Mean Squared Error on Test Set: {mse}\")\n",
        "print(f\"R-squared on Test Set: {r2}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOX_OSYLZAH7",
        "outputId": "ad2e74d8-4e24-492c-d535-d2d4429cc0c1"
      },
      "outputs": [],
      "source": [
        "X_for_vif = X_reduced.drop(columns=['const'], errors='ignore')\n",
        "\n",
        "# Ensure that the target variable 'stars' is not in the feature set\n",
        "assert 'stars' not in X_for_vif.columns, \"'stars' should not be in the feature set for VIF calculation.\"\n",
        "\n",
        "# Initialize an empty DataFrame to store VIF results\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"Feature\"] = X_for_vif.columns\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(X_for_vif.values, i) for i in range(X_for_vif.shape[1])]\n",
        "\n",
        "# Display the VIF data, filtering out features with low multicollinearity (VIF < 5)\n",
        "vif_filtered = vif_data[vif_data[\"VIF\"] > 5].sort_values(by=\"VIF\", ascending=False)\n",
        "print(vif_filtered)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 973
        },
        "id": "5UCwphuSXnmG",
        "outputId": "c174a354-8cd2-4535-d19f-d150513aa713"
      },
      "outputs": [],
      "source": [
        "correlation_matrix = df_restaurants_model.corr().round(2)\n",
        "\n",
        "# Create a heatmap with adjusted size and smaller annotation text\n",
        "fig = plt.figure(figsize=(12, 10))  # Increase the figure size\n",
        "sns.heatmap(correlation_matrix, cmap=\"coolwarm\", annot=True, annot_kws={\"size\":8}, fmt='.2f')\n",
        "\n",
        "# Display the plot\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fz9MhMb2ZHQ5",
        "outputId": "a4d92b2b-e1b2-4102-9a0a-0cc96a56db2c"
      },
      "outputs": [],
      "source": [
        "# Initialize the Random Forest Regressor\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# Calculate the mean squared error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "\n",
        "# Calculate feature importances\n",
        "importances = rf.feature_importances_\n",
        "\n",
        "# Ensure to use the feature names from the transformed dataset\n",
        "feature_importance = pd.DataFrame({'Feature': X_train.columns, 'Importance': importances})\n",
        "\n",
        "# Sort by importance\n",
        "feature_importance = feature_importance.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Display the feature importance\n",
        "print(feature_importance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qR0przHn2iH",
        "outputId": "b40b98ba-ba01-45ed-f9b4-6b7341257cf9"
      },
      "outputs": [],
      "source": [
        "# Dropping\n",
        "X_reduced = df_restaurants_model.drop(columns=['stars', 'appointment_only', 'coat_check', 'dogs', 'credit_card', 'good_for_groups', 'drive_thru', 'hours_weekend'], errors='ignore')\n",
        "\n",
        "# Creating new terms\n",
        "df_restaurants_model['delivery_drive_thru'] = df_restaurants_model['delivery'] * df_restaurants_model['drive_thru']\n",
        "\n",
        "# Add a constant to the model (intercept)\n",
        "X_reduced = sm.add_constant(X_reduced)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Perform the regression using statsmodels\n",
        "# Fit the model to the training data\n",
        "model = sm.OLS(y_train, X_train).fit()\n",
        "\n",
        "# Step 4: Get the summary of the model\n",
        "print(model.summary())\n",
        "\n",
        "# Step 5: Make predictions on the test set (optional)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model on the test set (optional)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Mean Squared Error on Test Set: {mse}\")\n",
        "print(f\"R-squared on Test Set: {r2}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buq4qFCdn2kw",
        "outputId": "2140cd56-ecf7-4031-ae8b-80592a7da640"
      },
      "outputs": [],
      "source": [
        "X_for_vif = X_reduced.drop(columns=['const'], errors='ignore')\n",
        "\n",
        "# Ensure that the target variable 'stars' is not in the feature set\n",
        "assert 'stars' not in X_for_vif.columns, \"'stars' should not be in the feature set for VIF calculation.\"\n",
        "\n",
        "# Initialize an empty DataFrame to store VIF results\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"Feature\"] = X_for_vif.columns\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(X_for_vif.values, i) for i in range(X_for_vif.shape[1])]\n",
        "\n",
        "# Display the VIF data, filtering out features with low multicollinearity (VIF < 5)\n",
        "vif_filtered = vif_data[vif_data[\"VIF\"] > 5].sort_values(by=\"VIF\", ascending=False)\n",
        "print(vif_filtered)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d76l-ySEn9Px",
        "outputId": "2e778f28-0ebc-49e1-913d-7613db88e265"
      },
      "outputs": [],
      "source": [
        "# Initialize the Random Forest Regressor\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# Calculate the mean squared error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "\n",
        "# Calculate feature importances\n",
        "importances = rf.feature_importances_\n",
        "\n",
        "# Ensure to use the feature names from the transformed dataset\n",
        "feature_importance = pd.DataFrame({'Feature': X_train.columns, 'Importance': importances})\n",
        "\n",
        "# Sort by importance\n",
        "feature_importance = feature_importance.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Display the feature importance\n",
        "print(feature_importance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5ME59TyrcR5"
      },
      "source": [
        "### Let's try Lasso and Ridge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uK2bluW8rcbR",
        "outputId": "2ba0212a-a36f-4891-c806-420a3c6221c1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import Lasso, Ridge\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Step 1: Prepare the data (you've already done this part)\n",
        "X_reduced = df_restaurants_model.drop(columns=['stars', 'appointment_only', 'coat_check', 'dogs', 'credit_card', 'good_for_groups', 'open_on_weekend', 'drive_thru'], errors='ignore')\n",
        "\n",
        "# Create the new interaction term\n",
        "df_restaurants_model['delivery_drive_thru'] = df_restaurants_model['delivery'] * df_restaurants_model['drive_thru']\n",
        "df_restaurants_model['hours_weekend'] = df_restaurants_model['hours_per_week'] * df_restaurants_model['open_on_weekend']\n",
        "\n",
        "# Step 2: Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_reduced_scaled = scaler.fit_transform(X_reduced)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_reduced_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Fit Lasso Regression\n",
        "lasso = Lasso(alpha=0.1, random_state=42)  # You can tune the alpha parameter\n",
        "lasso.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Fit Ridge Regression\n",
        "ridge = Ridge(alpha=1.0, random_state=42)  # You can tune the alpha parameter\n",
        "ridge.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Evaluate the Lasso model\n",
        "y_pred_lasso = lasso.predict(X_test)\n",
        "mse_lasso = mean_squared_error(y_test, y_pred_lasso)\n",
        "r2_lasso = r2_score(y_test, y_pred_lasso)\n",
        "\n",
        "print(\"Lasso Regression Results\")\n",
        "print(f\"Mean Squared Error: {mse_lasso}\")\n",
        "print(f\"R-squared: {r2_lasso}\")\n",
        "\n",
        "# Step 6: Evaluate the Ridge model\n",
        "y_pred_ridge = ridge.predict(X_test)\n",
        "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
        "r2_ridge = r2_score(y_test, y_pred_ridge)\n",
        "\n",
        "print(\"\\nRidge Regression Results\")\n",
        "print(f\"Mean Squared Error: {mse_ridge}\")\n",
        "print(f\"R-squared: {r2_ridge}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WDG2mlbrnd3",
        "outputId": "a3cb93b2-e76d-4212-b8e0-53b8f4dad76f"
      },
      "outputs": [],
      "source": [
        "X_for_vif = X_reduced.drop(columns=['const'], errors='ignore')\n",
        "\n",
        "# Ensure that the target variable 'stars' is not in the feature set\n",
        "assert 'stars' not in X_for_vif.columns, \"'stars' should not be in the feature set for VIF calculation.\"\n",
        "\n",
        "# Initialize an empty DataFrame to store VIF results\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"Feature\"] = X_for_vif.columns\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(X_for_vif.values, i) for i in range(X_for_vif.shape[1])]\n",
        "\n",
        "# Display the VIF data, filtering out features with low multicollinearity (VIF < 5)\n",
        "vif_filtered = vif_data[vif_data[\"VIF\"] > 5].sort_values(by=\"VIF\", ascending=False)\n",
        "print(vif_filtered)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVBuJpWVaw2u"
      },
      "source": [
        "## 4.6 Using recursive feature elimination"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABwOaRiqaHcj",
        "outputId": "fca5f26f-28dd-4fa7-e755-02cca37dd54d"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "model = LinearRegression()\n",
        "rfe = RFE(model, n_features_to_select=10)\n",
        "rfe = rfe.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Selected Features: {X_train.columns[rfe.support_]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqVTe4NyadQN",
        "outputId": "017a31ff-039a-48a4-c82c-0a357da694bd"
      },
      "outputs": [],
      "source": [
        "selected_features = X_train.columns[rfe.support_]\n",
        "X_train_selected = X_train[selected_features]\n",
        "X_test_selected = X_test[selected_features]\n",
        "\n",
        "# Retrain the model with selected features\n",
        "model.fit(X_train_selected, y_train)\n",
        "y_pred_selected = model.predict(X_test_selected)\n",
        "\n",
        "# Evaluate the model\n",
        "mse_selected = mean_squared_error(y_test, y_pred_selected)\n",
        "r2_selected = r2_score(y_test, y_pred_selected)\n",
        "\n",
        "print(f\"Mean Squared Error with Selected Features: {mse_selected}\")\n",
        "print(f\"R-squared with Selected Features: {r2_selected}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fu6N5-aEarKC",
        "outputId": "54b7ff9b-a9d0-4d7c-a6cd-aa68f72229c3"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "cv_scores = cross_val_score(model, X_train_selected, y_train, cv=5)\n",
        "print(f\"Cross-Validation Scores with Selected Features: {cv_scores}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCVmWeyDW72i"
      },
      "source": [
        "# 5 Testing different **models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MTuE0EfR1ED"
      },
      "source": [
        "Things to try: SGD regressor, k-neighbors (run grid search -> penalty and alpha), random forest, gradient-boosting regressor, xgboost, svm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77bgnwPuXDT0"
      },
      "source": [
        "## 5.1 SGD regressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sphmhhyYYlxh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pt1BE1fYy1x"
      },
      "source": [
        "First we run a grid search to optimize parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JpXRa85Y1XI"
      },
      "outputs": [],
      "source": [
        "param_grid = {\n",
        "    'alpha': [0.0001, 0.001, 0.01, 0.1],\n",
        "    'penalty': ['l2', 'l1', 'elasticnet'],\n",
        "    'max_iter': [1000, 2000, 5000],\n",
        "    'learning_rate': ['constant', 'optimal', 'invscaling', 'adaptive'],\n",
        "    'eta0': [0.01, 0.1, 1.0]\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9f7O9GGY8wS"
      },
      "source": [
        "We want to do this for X_reduced first"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FP3VX542Y7jP"
      },
      "outputs": [],
      "source": [
        "# Split your data into training and testing sets\n",
        "X_train_red, X_test_red, y_train_red, y_test_red = train_test_split(X_reduced, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the SGDRegressor\n",
        "sgd = SGDRegressor(random_state=42)\n",
        "\n",
        "# Set up the grid search\n",
        "grid_search_red = GridSearchCV(sgd, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "\n",
        "# Fit the model on the reduced dataset\n",
        "grid_search_red.fit(X_train_red, y_train_red)\n",
        "\n",
        "# Get the best model and parameters\n",
        "best_model_red = grid_search_red.best_estimator_\n",
        "print(\"Best parameters for X_reduced:\", grid_search_red.best_params_)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred_red = best_model_red.predict(X_test_red)\n",
        "mse_red = mean_squared_error(y_test_red, y_pred_red)\n",
        "r2_red = r2_score(y_test_red, y_pred_red)\n",
        "\n",
        "print(f\"X_reduced - Mean Squared Error: {mse_red}\")\n",
        "print(f\"X_reduced - R-squared: {r2_red}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EQ8CXNVY-qy"
      },
      "source": [
        "And then X_selected (from recursive feature selection)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5jbqxr3Y7qJ"
      },
      "outputs": [],
      "source": [
        "# Split your data into training and testing sets\n",
        "X_train_sel, X_test_sel, y_train_sel, y_test_sel = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the SGDRegressor\n",
        "sgd = SGDRegressor(random_state=42)\n",
        "\n",
        "# Set up the grid search\n",
        "grid_search_sel = GridSearchCV(sgd, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "\n",
        "# Fit the model on the selected dataset\n",
        "grid_search_sel.fit(X_train_sel, y_train_sel)\n",
        "\n",
        "# Get the best model and parameters\n",
        "best_model_sel = grid_search_sel.best_estimator_\n",
        "print(\"Best parameters for X_selected:\", grid_search_sel.best_params_)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred_sel = best_model_sel.predict(X_test_sel)\n",
        "mse_sel = mean_squared_error(y_test_sel, y_pred_sel)\n",
        "r2_sel = r2_score(y_test_sel, y_pred_sel)\n",
        "\n",
        "print(f\"X_selected - Mean Squared Error: {mse_sel}\")\n",
        "print(f\"X_selected - R-squared: {r2_sel}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLZDVKKwXIWI"
      },
      "source": [
        "## 5.2 K-neighbors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m71EaGFvXIY-"
      },
      "source": [
        "## 5.3 Random forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyLLOgC0XIbr"
      },
      "source": [
        "## 5.4 Gradient-boosting regressor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8klCNQHXIhm"
      },
      "source": [
        "## 5.5 Xgboost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKCC5KDhXT_D"
      },
      "source": [
        "## 5.6 SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05Zd9R54XUHs"
      },
      "source": [
        "## 5.7 Optimize across models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EciJYv6XSieF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
